{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "num_layers = 3 # newly added in this tutorials\n",
    "num_epochs = 100\n",
    "total_series_length = 50000\n",
    "truncated_backprop_length = 15 \n",
    "state_size = 4\n",
    "num_classes = 2\n",
    "echo_step = 3 \n",
    "batch_size = 5 # number of sample trained in a iteration\n",
    "num_batches = total_series_length//batch_size//truncated_backprop_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generateData():\n",
    "    x = np.array(np.random.choice(2, total_series_length, p=[0.5, 0.5]))\n",
    "    y = np.roll(x, echo_step)\n",
    "    y[0:echo_step] = 0\n",
    "\n",
    "    x = x.reshape((batch_size, -1))  # The first index changing slowest, subseries as rows\n",
    "    y = y.reshape((batch_size, -1))\n",
    "\n",
    "    return (x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LSTMStateTuple(c=<tf.Tensor 'strided_slice:0' shape=(5, 4) dtype=float32>, h=<tf.Tensor 'strided_slice_1:0' shape=(5, 4) dtype=float32>),\n",
       " LSTMStateTuple(c=<tf.Tensor 'strided_slice_2:0' shape=(5, 4) dtype=float32>, h=<tf.Tensor 'strided_slice_3:0' shape=(5, 4) dtype=float32>),\n",
       " LSTMStateTuple(c=<tf.Tensor 'strided_slice_4:0' shape=(5, 4) dtype=float32>, h=<tf.Tensor 'strided_slice_5:0' shape=(5, 4) dtype=float32>))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchX_placeholder = tf.placeholder(tf.float32, [batch_size, truncated_backprop_length])\n",
    "batchY_placeholder = tf.placeholder(tf.int32, [batch_size, truncated_backprop_length])\n",
    "\n",
    "\n",
    "# Please refers to \n",
    "# https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/rnn/LSTMStateTuple\n",
    "# cell_state = tf.placeholder(tf.float32, [batch_size, state_size])\n",
    "# hidden_state = tf.placeholder(tf.float32, [batch_size, state_size])\n",
    "# init_state = tf.contrib.rnn.LSTMStateTuple(cell_state, hidden_state)\n",
    "# print(\"init_state created by LSTMStateTuple:\", init_state)\n",
    "\n",
    "# the aboved code is commented out because we are going to replace the init_state as \n",
    "# following\n",
    "init_state = tf.placeholder(tf.float32, [num_layers, 2, batch_size, state_size])\n",
    "\n",
    "# Since the TF Multilayer-LSTM-API accepts the states as a tuple of LSTMTuples, \n",
    "# we need to first unstack the state, then there will be num_layers of \n",
    "# tensor in shape of [2, batch_size, state_size] \n",
    "state_per_layer_list = tf.unstack(init_state, axis=0) \n",
    "# Then, transform the state_per_layer_list into a list of LSTMStateTuple\n",
    "rnn_tuple_state = tuple(\n",
    "    [tf.contrib.rnn.LSTMStateTuple(\n",
    "            state_per_layer_list[idx][0], \n",
    "            state_per_layer_list[idx][1])\n",
    "     for idx in range(num_layers)\n",
    "    ]\n",
    ")\n",
    "\n",
    "rnn_tuple_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# W and b is useless in this example, because the tensorflow will create the \n",
    "# weight for the variable in the cell.\n",
    "W = tf.Variable(np.random.rand(state_size+1, state_size), dtype=tf.float32)\n",
    "b = tf.Variable(np.zeros((1, state_size)), dtype=tf.float32)\n",
    "\n",
    "\n",
    "# W2 and b2 is needed as we will use this to calculate the logits of the outputs\n",
    "W2 = tf.Variable(np.random.rand(state_size, num_classes), dtype=tf.float32)\n",
    "b2 = tf.Variable(np.zeros((1, num_classes)), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# batchX_placeholder is in shape of (batch_size, truncated_backprop_length)\n",
    "# Let's say batch_size=5, and truncated_backprop_length=15\n",
    "# If we unstack the batchX_placeholder, it will have 15 of (5,) tensor.\n",
    "# The rnn is trained based on this 5 batches with 15 timesteps each\n",
    "\n",
    "# inputs_series = tf.unstack(batchX_placeholder, axis=1)\n",
    "# ^ the above code is replaced by tf.split command\n",
    "# tf.split will split the batchX_placeholder evenly into truncated_backprop_length\n",
    "# number of tensor along the 1st-axis\n",
    "# e.g. batchX_placeholder is of shape (5, 30) and truncated_backprop_length is 3\n",
    "# then, there will be 10 tensors of shape (5, 3)\n",
    "\n",
    "# it is not used in this example\n",
    "if False:\n",
    "    inputs_series = tf.split(batchX_placeholder, 15, axis=1)\n",
    "    labels_series = tf.unstack(batchY_placeholder, axis=1)\n",
    "\n",
    "    print(inputs_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is basically the forward pass in the RNN.\n",
    "# For each input in the inputs_series, we concatenate it with the state value.\n",
    "# Then, we apply the same weights W and biases b on each of these input and\n",
    "# find the corresponding state value in this time series. \n",
    "\n",
    "if False: # for reference only\n",
    "    current_state = init_state\n",
    "    states_series = []\n",
    "\n",
    "    for current_input in inputs_series:\n",
    "        # (5,) -> (5,1) so as to concatenate with the state\n",
    "        current_input = tf.reshape(current_input, [batch_size, 1])\n",
    "\n",
    "        # Calculate the sum of affine transform\n",
    "        # By concatenating those two tensors you will only use one matrix \n",
    "        # multiplication.\n",
    "        input_and_state_concatenated = tf.concat([current_input, current_state], 1)\n",
    "\n",
    "        next_state = tf.tanh(tf.matmul(input_and_state_concatenated, W) + b)\n",
    "        states_series.append(next_state)\n",
    "        current_state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ExpandDims:0' shape=(5, 15, 1) dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.expand_dims(batchX_placeholder, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"rnn/transpose:0\", shape=(5, 15, 4), dtype=float32)\n",
      "(LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_2:0' shape=(5, 4) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_3:0' shape=(5, 4) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_4:0' shape=(5, 4) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_5:0' shape=(5, 4) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_6:0' shape=(5, 4) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_7:0' shape=(5, 4) dtype=float32>))\n"
     ]
    }
   ],
   "source": [
    "# forward pass implemented with the help of tensorflow\n",
    "# This code resembles the above \n",
    "# cell = tf.contrib.rnn.BasicRNNCell(num_units=state_size)\n",
    "\n",
    "# used for single layer\n",
    "# cell = tf.contrib.rnn.BasicLSTMCell(num_units=state_size, state_is_tuple=True)\n",
    "# states_series, current_state = tf.contrib.rnn.static_rnn(cell, inputs_series, init_state)\n",
    "\n",
    "\n",
    "# used for multi-layer\n",
    "# cell = tf.contrib.rnn.BasicLSTMCell(num_units=state_size, state_is_tuple=True)\n",
    "# cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "# states_series, current_state = tf.contrib.rnn.static_rnn(cell, \n",
    "#                                                        inputs_series, \n",
    "#                                                        initial_state=rnn_tuple_state)\n",
    "\n",
    "\n",
    "# dynamic_rnn function takes the batch inputs of shape \n",
    "# [batch_size, truncated_backprop_length, input_size]\n",
    "# therefore we need to expand a single dimension on the end.\n",
    "# The output will be the last state of every layer in the network as an LSTMStateTuple\n",
    "# stored in current_state, as well as a tensor states_series with the shape of\n",
    "# [batch_size, truncated_backprop_length, state_size] containing the hidden state of\n",
    "# the last layer across all timesteps\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=state_size, state_is_tuple=True)\n",
    "cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "states_series, current_state = tf.nn.dynamic_rnn(cell, \n",
    "                                                tf.expand_dims(batchX_placeholder, -1),\n",
    "                                               initial_state=rnn_tuple_state,\n",
    "                                                time_major=False) # time is not in 1st axis\n",
    "\n",
    "print (states_series)\n",
    "print (current_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_2:0' shape=(5, 4) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_3:0' shape=(5, 4) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_4:0' shape=(5, 4) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_5:0' shape=(5, 4) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_6:0' shape=(5, 4) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_7:0' shape=(5, 4) dtype=float32>))\n",
      "Tensor(\"Reshape:0\", shape=(75, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# reshape the states_series to [batch_size*truncated_backprop_length, state_size]\n",
    "states_series = tf.reshape(states_series, [-1, state_size])\n",
    "\n",
    "# each layer will return a state, therefore there will be 3 cell_states and hidden_states\n",
    "# Thus, in each timestep, the rnn will return 3 tuples of LSTM cell state.\n",
    "print(current_state)\n",
    "\n",
    "# The state_series will return only the last layer of the LSTM \n",
    "print (states_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# (75, 2)\n",
    "logits = tf.matmul(states_series, W2) + b2\n",
    "labels = tf.reshape(batchY_placeholder, [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculating loss\n",
    "\n",
    "# the logits originally is of shape [batch_size*truncated_bp, num_classes]\n",
    "# it is first reshaped into [batch_size, truncated_bp, num_classes]\n",
    "# then, unstack along the 1st axis, so the output will have 'truncated_bp' \n",
    "# number of tensor of shape [batch_size, num_classes]\n",
    "logits_series = tf.unstack(\n",
    "    tf.reshape(logits,[batch_size, truncated_backprop_length, num_classes]),\n",
    "    axis=1)\n",
    "predictions_series = [tf.nn.softmax(logit) for logit in logits_series]\n",
    "\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "\n",
    "train_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Calculating loss\n",
    "\n",
    "    # This calculate the logits of the series X fed intp the rnn by multiplying\n",
    "    # the hidden state neurons with weights and adding the biases. If the truncated\n",
    "    # length is of 15, then the length of logits series is also of 15.\n",
    "    logits_series = [tf.matmul(state, W2) + b2 for state in states_series]\n",
    "    predictions_series = [tf.nn.softmax(logits) for logits in logits_series]\n",
    "\n",
    "    losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\\\n",
    "             for logits, labels in zip(logits_series, labels_series)]\n",
    "\n",
    "    total_loss = tf.reduce_mean(losses)\n",
    "\n",
    "    train_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualizing the training\n",
    "\n",
    "def plot(loss_list, predictions_series, batchX, batchY):\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.cla()\n",
    "    plt.plot(loss_list)\n",
    "\n",
    "    for batch_series_idx in range(5):\n",
    "        one_hot_output_series = \\\n",
    "        np.array(predictions_series)[:, batch_series_idx, :]\n",
    "        single_output_series = \\\n",
    "        np.array([(1 if out[0] < 0.5 else 0) for out in one_hot_output_series])\n",
    "\n",
    "        plt.subplot(2, 3, batch_series_idx + 2)\n",
    "        plt.cla()\n",
    "        plt.axis([0, truncated_backprop_length, 0, 2])\n",
    "        left_offset = range(truncated_backprop_length)\n",
    "        plt.bar(left_offset, batchX[batch_series_idx, :], width=1, color=\"blue\")\n",
    "        plt.bar(left_offset, batchY[batch_series_idx, :] * 0.5, width=1, color=\"red\")\n",
    "        plt.bar(left_offset, single_output_series * 0.3, width=1, color=\"green\")\n",
    "\n",
    "    plt.draw()\n",
    "    plt.pause(0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e07fca7da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New data, epoch 0\n",
      "Step 0 Loss 0.693214\n",
      "Step 100 Loss 0.688469\n",
      "Step 200 Loss 0.678645\n",
      "Step 300 Loss 0.55753\n",
      "Step 400 Loss 0.562501\n",
      "Step 500 Loss 0.478465\n",
      "Step 600 Loss 0.367533\n",
      "New data, epoch 1\n",
      "Step 0 Loss 0.310649\n",
      "Step 100 Loss 0.00877084\n",
      "Step 200 Loss 0.00408865\n",
      "Step 300 Loss 0.00293484\n",
      "Step 400 Loss 0.00204762\n",
      "Step 500 Loss 0.00154591\n",
      "Step 600 Loss 0.00136973\n",
      "New data, epoch 2\n",
      "Step 0 Loss 0.302162\n",
      "Step 100 Loss 0.00103956\n",
      "Step 200 Loss 0.000960993\n",
      "Step 300 Loss 0.000909116\n",
      "Step 400 Loss 0.000748888\n",
      "Step 500 Loss 0.000705278\n",
      "Step 600 Loss 0.000609384\n",
      "New data, epoch 3\n",
      "Step 0 Loss 0.314657\n",
      "Step 100 Loss 0.000615921\n",
      "Step 200 Loss 0.000520673\n",
      "Step 300 Loss 0.000434954\n",
      "Step 400 Loss 0.000432143\n",
      "Step 500 Loss 0.000393571\n",
      "Step 600 Loss 0.000393609\n",
      "New data, epoch 4\n",
      "Step 0 Loss 0.417513\n",
      "Step 100 Loss 0.000363807\n",
      "Step 200 Loss 0.000408322\n",
      "Step 300 Loss 0.000368793\n",
      "Step 400 Loss 0.000338281\n",
      "Step 500 Loss 0.0003256\n",
      "Step 600 Loss 0.000340364\n",
      "New data, epoch 5\n",
      "Step 0 Loss 0.352023\n",
      "Step 100 Loss 0.000360312\n",
      "Step 200 Loss 0.000283065\n",
      "Step 300 Loss 0.000265977\n",
      "Step 400 Loss 0.000288407\n",
      "Step 500 Loss 0.000280851\n",
      "Step 600 Loss 0.000215693\n",
      "New data, epoch 6\n",
      "Step 0 Loss 0.183638\n",
      "Step 100 Loss 0.000199555\n",
      "Step 200 Loss 0.000201599\n",
      "Step 300 Loss 0.00022123\n",
      "Step 400 Loss 0.000219365\n",
      "Step 500 Loss 0.000186115\n",
      "Step 600 Loss 0.000198608\n",
      "New data, epoch 7\n",
      "Step 0 Loss 0.234259\n",
      "Step 100 Loss 0.00016874\n",
      "Step 200 Loss 0.000179986\n",
      "Step 300 Loss 0.000186721\n",
      "Step 400 Loss 0.000147948\n",
      "Step 500 Loss 0.000178884\n",
      "Step 600 Loss 0.000168364\n",
      "New data, epoch 8\n",
      "Step 0 Loss 0.293374\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    plt.ion()\n",
    "    plt.figure()\n",
    "    plt.show()\n",
    "    loss_list=[]\n",
    "    \n",
    "    for epoch_idx in range(num_epochs):\n",
    "        x,y = generateData()\n",
    "        # _current_cell_state = np.zeros((batch_size, state_size))\n",
    "        # _current_hidden_state = np.zeros((batch_size, state_size))\n",
    "        # The following _current_state is defined for the multi-layered LSTM\n",
    "        # It defines as followed:\n",
    "        # (1) the number of layers we have in the RNN model, within each layers,\n",
    "        # (2) there are 2 states - cell_state and hidden_state\n",
    "        # (3) Each state will simultaneously consider batch_size of input, and \n",
    "        # (4) the number of units of the state is of state_size\n",
    "        _current_state = np.zeros((num_layers, 2, batch_size, state_size))\n",
    "        \n",
    "        print(\"New data, epoch\", epoch_idx)\n",
    "        \n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * truncated_backprop_length\n",
    "            end_idx = start_idx + truncated_backprop_length\n",
    "            \n",
    "            batchX = x[:,start_idx:end_idx]\n",
    "            batchY = y[:,start_idx:end_idx]\n",
    "            \n",
    "            _total_loss, _train_step, _current_state, _predictions_series = \\\n",
    "            sess.run(\n",
    "                [total_loss, train_step, current_state, predictions_series],\n",
    "                feed_dict={\n",
    "                    batchX_placeholder: batchX,\n",
    "                    batchY_placeholder: batchY,\n",
    "                    init_state: _current_state\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            loss_list.append(_total_loss)\n",
    "            \n",
    "            if batch_idx%100 == 0:\n",
    "                print(\"Step\",batch_idx,\"Loss\", _total_loss)\n",
    "                plot(loss_list, _predictions_series, batchX, batchY)\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "current_input = inputs_series[0]\n",
    "current_input = tf.reshape(current_input, [batch_size, 1])\n",
    "current_state = init_state\n",
    "tf.concat([current_input, current_state], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
