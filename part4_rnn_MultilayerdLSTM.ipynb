{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "num_layers = 3 # newly added in this tutorials\n",
    "num_epochs = 100\n",
    "total_series_length = 50000\n",
    "truncated_backprop_length = 15 \n",
    "state_size = 4\n",
    "num_classes = 2\n",
    "echo_step = 3 \n",
    "batch_size = 5 # number of sample trained in a iteration\n",
    "num_batches = total_series_length//batch_size//truncated_backprop_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generateData():\n",
    "    x = np.array(np.random.choice(2, total_series_length, p=[0.5, 0.5]))\n",
    "    y = np.roll(x, echo_step)\n",
    "    y[0:echo_step] = 0\n",
    "\n",
    "    x = x.reshape((batch_size, -1))  # The first index changing slowest, subseries as rows\n",
    "    y = y.reshape((batch_size, -1))\n",
    "\n",
    "    return (x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LSTMStateTuple(c=<tf.Tensor 'strided_slice:0' shape=(5, 4) dtype=float32>, h=<tf.Tensor 'strided_slice_1:0' shape=(5, 4) dtype=float32>),\n",
       " LSTMStateTuple(c=<tf.Tensor 'strided_slice_2:0' shape=(5, 4) dtype=float32>, h=<tf.Tensor 'strided_slice_3:0' shape=(5, 4) dtype=float32>),\n",
       " LSTMStateTuple(c=<tf.Tensor 'strided_slice_4:0' shape=(5, 4) dtype=float32>, h=<tf.Tensor 'strided_slice_5:0' shape=(5, 4) dtype=float32>))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchX_placeholder = tf.placeholder(tf.float32, [batch_size, truncated_backprop_length])\n",
    "batchY_placeholder = tf.placeholder(tf.int32, [batch_size, truncated_backprop_length])\n",
    "\n",
    "\n",
    "# Please refers to \n",
    "# https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/rnn/LSTMStateTuple\n",
    "# cell_state = tf.placeholder(tf.float32, [batch_size, state_size])\n",
    "# hidden_state = tf.placeholder(tf.float32, [batch_size, state_size])\n",
    "# init_state = tf.contrib.rnn.LSTMStateTuple(cell_state, hidden_state)\n",
    "# print(\"init_state created by LSTMStateTuple:\", init_state)\n",
    "\n",
    "# the aboved code is commented out because we are going to replace the init_state as \n",
    "# following\n",
    "init_state = tf.placeholder(tf.float32, [num_layers, 2, batch_size, state_size])\n",
    "\n",
    "# Since the TF Multilayer-LSTM-API accepts the states as a tuple of LSTMTuples, \n",
    "# we need to first unstack the state, then there will be num_layers of \n",
    "# tensor in shape of [2, batch_size, state_size] \n",
    "state_per_layer_list = tf.unstack(init_state, axis=0) \n",
    "# Then, transform the state_per_layer_list into a list of LSTMStateTuple\n",
    "rnn_tuple_state = tuple(\n",
    "    [tf.contrib.rnn.LSTMStateTuple(\n",
    "            state_per_layer_list[idx][0], \n",
    "            state_per_layer_list[idx][1])\n",
    "     for idx in range(num_layers)\n",
    "    ]\n",
    ")\n",
    "\n",
    "rnn_tuple_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# W and b is useless in this example, because the tensorflow will create the \n",
    "# weight for the variable in the cell.\n",
    "W = tf.Variable(np.random.rand(state_size+1, state_size), dtype=tf.float32)\n",
    "b = tf.Variable(np.zeros((1, state_size)), dtype=tf.float32)\n",
    "\n",
    "\n",
    "# W2 and b2 is needed as we will use this to calculate the logits of the outputs\n",
    "W2 = tf.Variable(np.random.rand(state_size, num_classes), dtype=tf.float32)\n",
    "b2 = tf.Variable(np.zeros((1, num_classes)), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'split:0' shape=(5, 1) dtype=float32>, <tf.Tensor 'split:1' shape=(5, 1) dtype=float32>, <tf.Tensor 'split:2' shape=(5, 1) dtype=float32>, <tf.Tensor 'split:3' shape=(5, 1) dtype=float32>, <tf.Tensor 'split:4' shape=(5, 1) dtype=float32>, <tf.Tensor 'split:5' shape=(5, 1) dtype=float32>, <tf.Tensor 'split:6' shape=(5, 1) dtype=float32>, <tf.Tensor 'split:7' shape=(5, 1) dtype=float32>, <tf.Tensor 'split:8' shape=(5, 1) dtype=float32>, <tf.Tensor 'split:9' shape=(5, 1) dtype=float32>, <tf.Tensor 'split:10' shape=(5, 1) dtype=float32>, <tf.Tensor 'split:11' shape=(5, 1) dtype=float32>, <tf.Tensor 'split:12' shape=(5, 1) dtype=float32>, <tf.Tensor 'split:13' shape=(5, 1) dtype=float32>, <tf.Tensor 'split:14' shape=(5, 1) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "# batchX_placeholder is in shape of (batch_size, truncated_backprop_length)\n",
    "# Let's say batch_size=5, and truncated_backprop_length=15\n",
    "# If we unstack the batchX_placeholder, it will have 15 of (5,) tensor.\n",
    "# The rnn is trained based on this 5 batches with 15 timesteps each\n",
    "\n",
    "# inputs_series = tf.unstack(batchX_placeholder, axis=1)\n",
    "# ^ the above code is replaced by tf.split command\n",
    "# tf.split will split the batchX_placeholder evenly into truncated_backprop_length\n",
    "# number of tensor along the 1st-axis\n",
    "# e.g. batchX_placeholder is of shape (5, 30) and truncated_backprop_length is 3\n",
    "# then, there will be 10 tensors of shape (5, 3)\n",
    "inputs_series = tf.split(batchX_placeholder, 15, axis=1)\n",
    "labels_series = tf.unstack(batchY_placeholder, axis=1)\n",
    "\n",
    "print(inputs_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is basically the forward pass in the RNN.\n",
    "# For each input in the inputs_series, we concatenate it with the state value.\n",
    "# Then, we apply the same weights W and biases b on each of these input and\n",
    "# find the corresponding state value in this time series. \n",
    "\n",
    "if False: # for reference only\n",
    "    current_state = init_state\n",
    "    states_series = []\n",
    "\n",
    "    for current_input in inputs_series:\n",
    "        # (5,) -> (5,1) so as to concatenate with the state\n",
    "        current_input = tf.reshape(current_input, [batch_size, 1])\n",
    "\n",
    "        # Calculate the sum of affine transform\n",
    "        # By concatenating those two tensors you will only use one matrix \n",
    "        # multiplication.\n",
    "        input_and_state_concatenated = tf.concat([current_input, current_state], 1)\n",
    "\n",
    "        next_state = tf.tanh(tf.matmul(input_and_state_concatenated, W) + b)\n",
    "        states_series.append(next_state)\n",
    "        current_state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# forward pass implemented with the help of tensorflow\n",
    "# This code resembles the above \n",
    "# cell = tf.contrib.rnn.BasicRNNCell(num_units=state_size)\n",
    "\n",
    "# used for single layer\n",
    "# cell = tf.contrib.rnn.BasicLSTMCell(num_units=state_size, state_is_tuple=True)\n",
    "# states_series, current_state = tf.contrib.rnn.static_rnn(cell, inputs_series, init_state)\n",
    "\n",
    "\n",
    "# used for multi-layer\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=state_size, state_is_tuple=True)\n",
    "cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "states_series, current_state = tf.contrib.rnn.static_rnn(cell, \n",
    "                                                       inputs_series, \n",
    "                                                       initial_state=rnn_tuple_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(LSTMStateTuple(c=<tf.Tensor 'rnn/multi_rnn_cell_14/cell_0/basic_lstm_cell/add_1:0' shape=(5, 4) dtype=float32>, h=<tf.Tensor 'rnn/multi_rnn_cell_14/cell_0/basic_lstm_cell/mul_2:0' shape=(5, 4) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'rnn/multi_rnn_cell_14/cell_1/basic_lstm_cell/add_1:0' shape=(5, 4) dtype=float32>, h=<tf.Tensor 'rnn/multi_rnn_cell_14/cell_1/basic_lstm_cell/mul_2:0' shape=(5, 4) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'rnn/multi_rnn_cell_14/cell_2/basic_lstm_cell/add_1:0' shape=(5, 4) dtype=float32>, h=<tf.Tensor 'rnn/multi_rnn_cell_14/cell_2/basic_lstm_cell/mul_2:0' shape=(5, 4) dtype=float32>))\n",
      "[<tf.Tensor 'rnn/multi_rnn_cell/cell_2/basic_lstm_cell/mul_2:0' shape=(5, 4) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_1/cell_2/basic_lstm_cell/mul_2:0' shape=(5, 4) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_2/cell_2/basic_lstm_cell/mul_2:0' shape=(5, 4) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_3/cell_2/basic_lstm_cell/mul_2:0' shape=(5, 4) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_4/cell_2/basic_lstm_cell/mul_2:0' shape=(5, 4) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_5/cell_2/basic_lstm_cell/mul_2:0' shape=(5, 4) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_6/cell_2/basic_lstm_cell/mul_2:0' shape=(5, 4) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_7/cell_2/basic_lstm_cell/mul_2:0' shape=(5, 4) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_8/cell_2/basic_lstm_cell/mul_2:0' shape=(5, 4) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_9/cell_2/basic_lstm_cell/mul_2:0' shape=(5, 4) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_10/cell_2/basic_lstm_cell/mul_2:0' shape=(5, 4) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_11/cell_2/basic_lstm_cell/mul_2:0' shape=(5, 4) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_12/cell_2/basic_lstm_cell/mul_2:0' shape=(5, 4) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_13/cell_2/basic_lstm_cell/mul_2:0' shape=(5, 4) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_14/cell_2/basic_lstm_cell/mul_2:0' shape=(5, 4) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "# each layer will return a state, therefore there will be 3 cell_states and hidden_states\n",
    "# Thus, in each timestep, the rnn will return 3 tuples of LSTM cell state.\n",
    "print(current_state)\n",
    "\n",
    "# The state_series will return only the last layer of the LSTM \n",
    "print (states_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculating loss\n",
    "\n",
    "# This calculate the logits of the series X fed intp the rnn by multiplying\n",
    "# the hidden state neurons with weights and adding the biases. If the truncated\n",
    "# length is of 15, then the length of logits series is also of 15.\n",
    "logits_series = [tf.matmul(state, W2) + b2 for state in states_series]\n",
    "predictions_series = [tf.nn.softmax(logits) for logits in logits_series]\n",
    "\n",
    "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\\\n",
    "         for logits, labels in zip(logits_series, labels_series)]\n",
    "\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "\n",
    "train_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualizing the training\n",
    "\n",
    "def plot(loss_list, predictions_series, batchX, batchY):\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.cla()\n",
    "    plt.plot(loss_list)\n",
    "\n",
    "    for batch_series_idx in range(5):\n",
    "        one_hot_output_series = \\\n",
    "        np.array(predictions_series)[:, batch_series_idx, :]\n",
    "        single_output_series = \\\n",
    "        np.array([(1 if out[0] < 0.5 else 0) for out in one_hot_output_series])\n",
    "\n",
    "        plt.subplot(2, 3, batch_series_idx + 2)\n",
    "        plt.cla()\n",
    "        plt.axis([0, truncated_backprop_length, 0, 2])\n",
    "        left_offset = range(truncated_backprop_length)\n",
    "        plt.bar(left_offset, batchX[batch_series_idx, :], width=1, color=\"blue\")\n",
    "        plt.bar(left_offset, batchY[batch_series_idx, :] * 0.5, width=1, color=\"red\")\n",
    "        plt.bar(left_offset, single_output_series * 0.3, width=1, color=\"green\")\n",
    "\n",
    "    plt.draw()\n",
    "    plt.pause(0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1872935eb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New data, epoch 0\n",
      "Step 0 Loss 0.693052\n",
      "Step 100 Loss 0.684805\n",
      "Step 200 Loss 0.578267\n",
      "Step 300 Loss 0.478715\n",
      "Step 400 Loss 0.330643\n",
      "Step 500 Loss 0.0281259\n",
      "Step 600 Loss 0.00912669\n",
      "New data, epoch 1\n",
      "Step 0 Loss 0.55962\n",
      "Step 100 Loss 0.00524076\n",
      "Step 200 Loss 0.00378288\n",
      "Step 300 Loss 0.0030171\n",
      "Step 400 Loss 0.00230438\n",
      "Step 500 Loss 0.00232687\n",
      "Step 600 Loss 0.00183346\n",
      "New data, epoch 2\n",
      "Step 0 Loss 0.524574\n",
      "Step 100 Loss 0.00159937\n",
      "Step 200 Loss 0.00155124\n",
      "Step 300 Loss 0.00104901\n",
      "Step 400 Loss 0.00123688\n",
      "Step 500 Loss 0.00123167\n",
      "Step 600 Loss 0.00129611\n",
      "New data, epoch 3\n",
      "Step 0 Loss 0.3745\n",
      "Step 100 Loss 0.000696863\n",
      "Step 200 Loss 0.00083152\n",
      "Step 300 Loss 0.000495405\n",
      "Step 400 Loss 0.000502525\n",
      "Step 500 Loss 0.000583262\n",
      "Step 600 Loss 0.000623307\n",
      "New data, epoch 4\n",
      "Step 0 Loss 0.896701\n",
      "Step 100 Loss 0.000567448\n",
      "Step 200 Loss 0.000773069\n",
      "Step 300 Loss 0.000507078\n",
      "Step 400 Loss 0.00060116\n",
      "Step 500 Loss 0.000587442\n",
      "Step 600 Loss 0.000575203\n",
      "New data, epoch 5\n",
      "Step 0 Loss 0.633691\n",
      "Step 100 Loss 0.000585937\n",
      "Step 200 Loss 0.00049878\n",
      "Step 300 Loss 0.000374157\n",
      "Step 400 Loss 0.000577502\n",
      "Step 500 Loss 0.000367227\n",
      "Step 600 Loss 0.000387067\n",
      "New data, epoch 6\n",
      "Step 0 Loss 0.635643\n",
      "Step 100 Loss 0.00166716\n",
      "Step 200 Loss 0.000835953\n",
      "Step 300 Loss 0.000843011\n",
      "Step 400 Loss 0.000651568\n",
      "Step 500 Loss 0.000779552\n",
      "Step 600 Loss 0.000527847\n",
      "New data, epoch 7\n",
      "Step 0 Loss 0.847006\n",
      "Step 100 Loss 0.000563908\n",
      "Step 200 Loss 0.000598315\n",
      "Step 300 Loss 0.000628566\n",
      "Step 400 Loss 0.00066951\n",
      "Step 500 Loss 0.000420368\n",
      "Step 600 Loss 0.000421179\n",
      "New data, epoch 8\n",
      "Step 0 Loss 0.587583\n",
      "Step 100 Loss 0.000511984\n",
      "Step 200 Loss 0.000421859\n",
      "Step 300 Loss 0.000508087\n",
      "Step 400 Loss 0.000476273\n",
      "Step 500 Loss 0.000418161\n",
      "Step 600 Loss 0.000364133\n",
      "New data, epoch 9\n",
      "Step 0 Loss 0.247347\n",
      "Step 100 Loss 0.000338971\n",
      "Step 200 Loss 0.000331145\n",
      "Step 300 Loss 0.000290879\n",
      "Step 400 Loss 0.000306964\n",
      "Step 500 Loss 0.000337265\n",
      "Step 600 Loss 0.000251988\n",
      "New data, epoch 10\n",
      "Step 0 Loss 0.724141\n",
      "Step 100 Loss 0.000264204\n",
      "Step 200 Loss 0.000299702\n",
      "Step 300 Loss 0.000235634\n",
      "Step 400 Loss 0.000204404\n",
      "Step 500 Loss 0.000278492\n",
      "Step 600 Loss 0.000236878\n",
      "New data, epoch 11\n",
      "Step 0 Loss 0.450115\n",
      "Step 100 Loss 0.000238651\n",
      "Step 200 Loss 0.000261907\n",
      "Step 300 Loss 0.000335081\n",
      "Step 400 Loss 0.000286878\n",
      "Step 500 Loss 0.000262801\n",
      "Step 600 Loss 0.000295356\n",
      "New data, epoch 12\n",
      "Step 0 Loss 0.527462\n",
      "Step 100 Loss 0.000270073\n",
      "Step 200 Loss 0.000237535\n",
      "Step 300 Loss 0.000282104\n",
      "Step 400 Loss 0.000251098\n",
      "Step 500 Loss 0.0002293\n",
      "Step 600 Loss 0.000235212\n",
      "New data, epoch 13\n",
      "Step 0 Loss 0.183005\n",
      "Step 100 Loss 0.000254121\n",
      "Step 200 Loss 0.000214299\n",
      "Step 300 Loss 0.000270586\n",
      "Step 400 Loss 0.000167653\n",
      "Step 500 Loss 0.000236974\n",
      "Step 600 Loss 0.000202339\n",
      "New data, epoch 14\n",
      "Step 0 Loss 0.47424\n",
      "Step 100 Loss 0.000213777\n",
      "Step 200 Loss 0.000251651\n",
      "Step 300 Loss 0.000201682\n",
      "Step 400 Loss 0.000192585\n",
      "Step 500 Loss 0.000226424\n",
      "Step 600 Loss 0.000221533\n",
      "New data, epoch 15\n",
      "Step 0 Loss 0.388446\n",
      "Step 100 Loss 0.00023191\n",
      "Step 200 Loss 0.000273165\n",
      "Step 300 Loss 0.000143604\n",
      "Step 400 Loss 0.000171799\n",
      "Step 500 Loss 0.000144744\n",
      "Step 600 Loss 0.000264916\n",
      "New data, epoch 16\n",
      "Step 0 Loss 0.342361\n",
      "Step 100 Loss 0.000217678\n",
      "Step 200 Loss 0.000193232\n",
      "Step 300 Loss 0.0001744\n",
      "Step 400 Loss 0.0002413\n",
      "Step 500 Loss 0.000175461\n",
      "Step 600 Loss 0.000137309\n",
      "New data, epoch 17\n",
      "Step 0 Loss 0.34093\n",
      "Step 100 Loss 0.000125598\n",
      "Step 200 Loss 0.000215458\n",
      "Step 300 Loss 0.000152768\n",
      "Step 400 Loss 0.000147112\n",
      "Step 500 Loss 0.000191329\n",
      "Step 600 Loss 0.000140937\n",
      "New data, epoch 18\n",
      "Step 0 Loss 0.185342\n",
      "Step 100 Loss 0.000152049\n",
      "Step 200 Loss 0.000187726\n",
      "Step 300 Loss 0.000165103\n",
      "Step 400 Loss 0.000139739\n",
      "Step 500 Loss 0.000175891\n",
      "Step 600 Loss 0.000176395\n",
      "New data, epoch 19\n",
      "Step 0 Loss 0.799811\n",
      "Step 100 Loss 0.000191714\n",
      "Step 200 Loss 0.000159831\n",
      "Step 300 Loss 0.000173987\n",
      "Step 400 Loss 0.00016464\n",
      "Step 500 Loss 0.000185982\n",
      "Step 600 Loss 0.000177957\n",
      "New data, epoch 20\n",
      "Step 0 Loss 0.390753\n",
      "Step 100 Loss 0.000178222\n",
      "Step 200 Loss 0.00018695\n",
      "Step 300 Loss 0.000191678\n",
      "Step 400 Loss 0.000177773\n",
      "Step 500 Loss 0.000193876\n",
      "Step 600 Loss 0.000154985\n",
      "New data, epoch 21\n",
      "Step 0 Loss 0.252725\n",
      "Step 100 Loss 0.000167682\n",
      "Step 200 Loss 0.00016891\n",
      "Step 300 Loss 0.000132407\n",
      "Step 400 Loss 0.000114424\n",
      "Step 500 Loss 0.000157878\n",
      "Step 600 Loss 0.00012833\n",
      "New data, epoch 22\n",
      "Step 0 Loss 0.305868\n",
      "Step 100 Loss 0.000151511\n",
      "Step 200 Loss 0.000150441\n",
      "Step 300 Loss 0.000181233\n",
      "Step 400 Loss 0.000160916\n",
      "Step 500 Loss 0.000148205\n",
      "Step 600 Loss 0.000106236\n",
      "New data, epoch 23\n",
      "Step 0 Loss 0.622633\n",
      "Step 100 Loss 0.00013104\n",
      "Step 200 Loss 0.000108688\n",
      "Step 300 Loss 0.000117613\n",
      "Step 400 Loss 0.00014902\n",
      "Step 500 Loss 0.000143219\n",
      "Step 600 Loss 0.000146489\n",
      "New data, epoch 24\n",
      "Step 0 Loss 0.356466\n",
      "Step 100 Loss 0.000199128\n",
      "Step 200 Loss 0.000191754\n",
      "Step 300 Loss 0.000123023\n",
      "Step 400 Loss 0.000142101\n",
      "Step 500 Loss 0.000140706\n",
      "Step 600 Loss 0.000111007\n",
      "New data, epoch 25\n",
      "Step 0 Loss 0.316565\n",
      "Step 100 Loss 0.000179579\n",
      "Step 200 Loss 0.000142325\n",
      "Step 300 Loss 0.000112112\n",
      "Step 400 Loss 0.000131139\n",
      "Step 500 Loss 0.000182034\n",
      "Step 600 Loss 0.000149294\n",
      "New data, epoch 26\n",
      "Step 0 Loss 0.6942\n",
      "Step 100 Loss 0.000122344\n",
      "Step 200 Loss 0.000168072\n",
      "Step 300 Loss 0.000118978\n",
      "Step 400 Loss 0.000132255\n",
      "Step 500 Loss 0.000130349\n",
      "Step 600 Loss 0.000132876\n",
      "New data, epoch 27\n",
      "Step 0 Loss 0.462982\n",
      "Step 100 Loss 0.000163583\n",
      "Step 200 Loss 0.00011496\n",
      "Step 300 Loss 0.000146047\n",
      "Step 400 Loss 0.000132663\n",
      "Step 500 Loss 0.000144052\n",
      "Step 600 Loss 0.00013011\n",
      "New data, epoch 28\n",
      "Step 0 Loss 0.684236\n",
      "Step 100 Loss 0.000159231\n",
      "Step 200 Loss 0.000144644\n",
      "Step 300 Loss 0.000174805\n",
      "Step 400 Loss 0.000133541\n",
      "Step 500 Loss 0.000119889\n",
      "Step 600 Loss 0.000140692\n",
      "New data, epoch 29\n",
      "Step 0 Loss 0.867034\n",
      "Step 100 Loss 0.000156282\n",
      "Step 200 Loss 0.000139661\n",
      "Step 300 Loss 0.000136862\n",
      "Step 400 Loss 0.000139244\n",
      "Step 500 Loss 0.000139093\n",
      "Step 600 Loss 0.00011464\n",
      "New data, epoch 30\n",
      "Step 0 Loss 0.626938\n",
      "Step 100 Loss 0.00017195\n",
      "Step 200 Loss 0.000195648\n",
      "Step 300 Loss 0.000171062\n",
      "Step 400 Loss 0.000151821\n",
      "Step 500 Loss 0.000150352\n",
      "Step 600 Loss 0.000152563\n",
      "New data, epoch 31\n",
      "Step 0 Loss 0.208894\n",
      "Step 100 Loss 0.000108805\n",
      "Step 200 Loss 0.000105492\n",
      "Step 300 Loss 0.000117057\n",
      "Step 400 Loss 8.78164e-05\n",
      "Step 500 Loss 0.000116084\n",
      "Step 600 Loss 9.61311e-05\n",
      "New data, epoch 32\n",
      "Step 0 Loss 0.578482\n",
      "Step 100 Loss 0.000121409\n",
      "Step 200 Loss 0.000106636\n",
      "Step 300 Loss 0.000111319\n",
      "Step 400 Loss 0.000112169\n",
      "Step 500 Loss 0.000105652\n",
      "Step 600 Loss 0.00010906\n",
      "New data, epoch 33\n",
      "Step 0 Loss 0.171088\n",
      "Step 100 Loss 9.15624e-05\n",
      "Step 200 Loss 0.000110669\n",
      "Step 300 Loss 0.000109057\n",
      "Step 400 Loss 0.000113308\n",
      "Step 500 Loss 0.000116123\n",
      "Step 600 Loss 9.60218e-05\n",
      "New data, epoch 34\n",
      "Step 0 Loss 0.715181\n",
      "Step 100 Loss 0.000116901\n",
      "Step 200 Loss 0.000115726\n",
      "Step 300 Loss 0.00011931\n",
      "Step 400 Loss 0.000116595\n",
      "Step 500 Loss 9.02688e-05\n",
      "Step 600 Loss 9.66652e-05\n",
      "New data, epoch 35\n",
      "Step 0 Loss 0.636961\n",
      "Step 100 Loss 0.000119773\n",
      "Step 200 Loss 0.000127347\n",
      "Step 300 Loss 0.000151645\n",
      "Step 400 Loss 0.000124504\n",
      "Step 500 Loss 0.000100914\n",
      "Step 600 Loss 0.000124794\n",
      "New data, epoch 36\n",
      "Step 0 Loss 0.323451\n",
      "Step 100 Loss 0.000132569\n",
      "Step 200 Loss 0.000159634\n",
      "Step 300 Loss 0.000121813\n",
      "Step 400 Loss 0.000131145\n",
      "Step 500 Loss 0.000103257\n",
      "Step 600 Loss 0.000108035\n",
      "New data, epoch 37\n",
      "Step 0 Loss 0.550875\n",
      "Step 100 Loss 7.25077e-05\n",
      "Step 200 Loss 8.99168e-05\n",
      "Step 300 Loss 0.000122958\n",
      "Step 400 Loss 0.000124082\n",
      "Step 500 Loss 0.00011008\n",
      "Step 600 Loss 9.96205e-05\n",
      "New data, epoch 38\n",
      "Step 0 Loss 0.426426\n",
      "Step 100 Loss 0.000126435\n",
      "Step 200 Loss 0.00012948\n",
      "Step 300 Loss 0.000141891\n",
      "Step 400 Loss 0.000142243\n",
      "Step 500 Loss 0.000135555\n",
      "Step 600 Loss 0.000125148\n",
      "New data, epoch 39\n",
      "Step 0 Loss 0.392197\n",
      "Step 100 Loss 0.00012066\n",
      "Step 200 Loss 0.000114672\n",
      "Step 300 Loss 0.000107064\n",
      "Step 400 Loss 8.87477e-05\n",
      "Step 500 Loss 9.90353e-05\n",
      "Step 600 Loss 0.000100145\n",
      "New data, epoch 40\n",
      "Step 0 Loss 0.377921\n",
      "Step 100 Loss 0.000153437\n",
      "Step 200 Loss 0.000116679\n",
      "Step 300 Loss 0.000132955\n",
      "Step 400 Loss 0.000109106\n",
      "Step 500 Loss 0.000115786\n",
      "Step 600 Loss 0.000102404\n",
      "New data, epoch 41\n",
      "Step 0 Loss 0.222813\n",
      "Step 100 Loss 9.77884e-05\n",
      "Step 200 Loss 0.0001077\n",
      "Step 300 Loss 0.000103492\n",
      "Step 400 Loss 0.000101212\n",
      "Step 500 Loss 0.000107874\n",
      "Step 600 Loss 8.30879e-05\n",
      "New data, epoch 42\n",
      "Step 0 Loss 0.466513\n",
      "Step 100 Loss 9.95762e-05\n",
      "Step 200 Loss 0.000100479\n",
      "Step 300 Loss 0.000119295\n",
      "Step 400 Loss 0.00011364\n",
      "Step 500 Loss 0.00010006\n",
      "Step 600 Loss 0.000101736\n",
      "New data, epoch 43\n",
      "Step 0 Loss 0.405784\n",
      "Step 100 Loss 0.000128212\n",
      "Step 200 Loss 8.97144e-05\n",
      "Step 300 Loss 0.000111722\n",
      "Step 400 Loss 0.000121394\n",
      "Step 500 Loss 0.000124143\n",
      "Step 600 Loss 0.000106359\n",
      "New data, epoch 44\n",
      "Step 0 Loss 0.411605\n",
      "Step 100 Loss 0.000120512\n",
      "Step 200 Loss 9.05215e-05\n",
      "Step 300 Loss 8.95194e-05\n",
      "Step 400 Loss 0.000108083\n",
      "Step 500 Loss 0.000105292\n",
      "Step 600 Loss 8.04474e-05\n",
      "New data, epoch 45\n",
      "Step 0 Loss 0.165933\n",
      "Step 100 Loss 5.873e-05\n",
      "Step 200 Loss 0.000127687\n",
      "Step 300 Loss 0.000109926\n",
      "Step 400 Loss 0.000118547\n",
      "Step 500 Loss 0.000102415\n",
      "Step 600 Loss 8.85155e-05\n",
      "New data, epoch 46\n",
      "Step 0 Loss 0.26766\n",
      "Step 100 Loss 0.000117761\n",
      "Step 200 Loss 8.67209e-05\n",
      "Step 300 Loss 0.000103115\n",
      "Step 400 Loss 9.84913e-05\n",
      "Step 500 Loss 0.000133126\n",
      "Step 600 Loss 0.000104593\n",
      "New data, epoch 47\n",
      "Step 0 Loss 0.254542\n",
      "Step 100 Loss 9.90704e-05\n",
      "Step 200 Loss 9.92163e-05\n",
      "Step 300 Loss 0.000112339\n",
      "Step 400 Loss 9.53441e-05\n",
      "Step 500 Loss 9.45227e-05\n",
      "Step 600 Loss 9.44019e-05\n",
      "New data, epoch 48\n",
      "Step 0 Loss 0.178266\n",
      "Step 100 Loss 0.000109578\n",
      "Step 200 Loss 0.000101482\n",
      "Step 300 Loss 9.54605e-05\n",
      "Step 400 Loss 9.32554e-05\n",
      "Step 500 Loss 7.50904e-05\n",
      "Step 600 Loss 8.0807e-05\n",
      "New data, epoch 49\n",
      "Step 0 Loss 0.407928\n",
      "Step 100 Loss 9.01271e-05\n",
      "Step 200 Loss 9.36087e-05\n",
      "Step 300 Loss 0.000101761\n",
      "Step 400 Loss 0.000112233\n",
      "Step 500 Loss 0.000101383\n",
      "Step 600 Loss 9.06003e-05\n",
      "New data, epoch 50\n",
      "Step 0 Loss 0.323473\n",
      "Step 100 Loss 8.94129e-05\n",
      "Step 200 Loss 0.000108059\n",
      "Step 300 Loss 9.86463e-05\n",
      "Step 400 Loss 9.84414e-05\n",
      "Step 500 Loss 7.47994e-05\n",
      "Step 600 Loss 8.63287e-05\n",
      "New data, epoch 51\n",
      "Step 0 Loss 0.178158\n",
      "Step 100 Loss 0.000113944\n",
      "Step 200 Loss 0.000119778\n",
      "Step 300 Loss 0.00010839\n",
      "Step 400 Loss 9.45397e-05\n",
      "Step 500 Loss 8.31657e-05\n",
      "Step 600 Loss 8.61628e-05\n",
      "New data, epoch 52\n",
      "Step 0 Loss 0.253117\n",
      "Step 100 Loss 0.00013189\n",
      "Step 200 Loss 0.000112605\n",
      "Step 300 Loss 6.93693e-05\n",
      "Step 400 Loss 7.05796e-05\n",
      "Step 500 Loss 6.68827e-05\n",
      "Step 600 Loss 9.94348e-05\n",
      "New data, epoch 53\n",
      "Step 0 Loss 0.254717\n",
      "Step 100 Loss 0.0001241\n",
      "Step 200 Loss 0.000106119\n",
      "Step 300 Loss 7.65636e-05\n",
      "Step 400 Loss 8.03653e-05\n",
      "Step 500 Loss 9.00284e-05\n",
      "Step 600 Loss 7.46678e-05\n",
      "New data, epoch 54\n",
      "Step 0 Loss 0.259774\n",
      "Step 100 Loss 8.64125e-05\n",
      "Step 200 Loss 0.000102037\n",
      "Step 300 Loss 8.08804e-05\n",
      "Step 400 Loss 8.77511e-05\n",
      "Step 500 Loss 7.64495e-05\n",
      "Step 600 Loss 6.9317e-05\n",
      "New data, epoch 55\n",
      "Step 0 Loss 0.35154\n",
      "Step 100 Loss 8.00751e-05\n",
      "Step 200 Loss 8.18791e-05\n",
      "Step 300 Loss 8.24984e-05\n",
      "Step 400 Loss 8.38051e-05\n",
      "Step 500 Loss 7.71212e-05\n",
      "Step 600 Loss 7.60921e-05\n",
      "New data, epoch 56\n",
      "Step 0 Loss 0.247258\n",
      "Step 100 Loss 9.04236e-05\n",
      "Step 200 Loss 8.44724e-05\n",
      "Step 300 Loss 7.08832e-05\n",
      "Step 400 Loss 8.18592e-05\n",
      "Step 500 Loss 7.83909e-05\n",
      "Step 600 Loss 7.34919e-05\n",
      "New data, epoch 57\n",
      "Step 0 Loss 0.30575\n",
      "Step 100 Loss 6.67108e-05\n",
      "Step 200 Loss 8.90013e-05\n",
      "Step 300 Loss 8.79989e-05\n",
      "Step 400 Loss 6.56573e-05\n",
      "Step 500 Loss 7.59742e-05\n",
      "Step 600 Loss 6.94279e-05\n",
      "New data, epoch 58\n",
      "Step 0 Loss 0.38471\n",
      "Step 100 Loss 9.68972e-05\n",
      "Step 200 Loss 8.56341e-05\n",
      "Step 300 Loss 7.61539e-05\n",
      "Step 400 Loss 7.71424e-05\n",
      "Step 500 Loss 9.63815e-05\n",
      "Step 600 Loss 8.27887e-05\n",
      "New data, epoch 59\n",
      "Step 0 Loss 0.325494\n",
      "Step 100 Loss 8.12827e-05\n",
      "Step 200 Loss 8.76781e-05\n",
      "Step 300 Loss 7.43795e-05\n",
      "Step 400 Loss 8.09954e-05\n",
      "Step 500 Loss 6.80899e-05\n",
      "Step 600 Loss 6.47924e-05\n",
      "New data, epoch 60\n",
      "Step 0 Loss 0.349657\n",
      "Step 100 Loss 8.1051e-05\n",
      "Step 200 Loss 7.3967e-05\n",
      "Step 300 Loss 9.50286e-05\n",
      "Step 400 Loss 8.43092e-05\n",
      "Step 500 Loss 7.1437e-05\n",
      "Step 600 Loss 7.34009e-05\n",
      "New data, epoch 61\n",
      "Step 0 Loss 0.256714\n",
      "Step 100 Loss 8.19839e-05\n",
      "Step 200 Loss 0.000121047\n",
      "Step 300 Loss 0.000137072\n",
      "Step 400 Loss 0.000105105\n",
      "Step 500 Loss 8.15871e-05\n",
      "Step 600 Loss 7.35054e-05\n",
      "New data, epoch 62\n",
      "Step 0 Loss 0.233681\n",
      "Step 100 Loss 8.77005e-05\n",
      "Step 200 Loss 7.683e-05\n",
      "Step 300 Loss 7.33489e-05\n",
      "Step 400 Loss 7.97864e-05\n",
      "Step 500 Loss 6.00245e-05\n",
      "Step 600 Loss 7.16243e-05\n",
      "New data, epoch 63\n",
      "Step 0 Loss 0.204444\n",
      "Step 100 Loss 6.97803e-05\n",
      "Step 200 Loss 6.23355e-05\n",
      "Step 300 Loss 7.10765e-05\n",
      "Step 400 Loss 7.21888e-05\n",
      "Step 500 Loss 6.51455e-05\n",
      "Step 600 Loss 6.97768e-05\n",
      "New data, epoch 64\n",
      "Step 0 Loss 0.189416\n",
      "Step 100 Loss 6.73738e-05\n",
      "Step 200 Loss 5.04726e-05\n",
      "Step 300 Loss 5.81852e-05\n",
      "Step 400 Loss 5.41107e-05\n",
      "Step 500 Loss 5.95284e-05\n",
      "Step 600 Loss 6.07686e-05\n",
      "New data, epoch 65\n",
      "Step 0 Loss 0.156871\n",
      "Step 100 Loss 6.47241e-05\n",
      "Step 200 Loss 6.3678e-05\n",
      "Step 300 Loss 6.49976e-05\n",
      "Step 400 Loss 6.20797e-05\n",
      "Step 500 Loss 6.08371e-05\n",
      "Step 600 Loss 5.43271e-05\n",
      "New data, epoch 66\n",
      "Step 0 Loss 0.177126\n",
      "Step 100 Loss 9.36836e-05\n",
      "Step 200 Loss 8.61348e-05\n",
      "Step 300 Loss 7.11011e-05\n",
      "Step 400 Loss 6.4014e-05\n",
      "Step 500 Loss 6.63956e-05\n",
      "Step 600 Loss 5.79215e-05\n",
      "New data, epoch 67\n",
      "Step 0 Loss 0.250031\n",
      "Step 100 Loss 7.23288e-05\n",
      "Step 200 Loss 7.64246e-05\n",
      "Step 300 Loss 7.97179e-05\n",
      "Step 400 Loss 7.23332e-05\n",
      "Step 500 Loss 5.90735e-05\n",
      "Step 600 Loss 6.65191e-05\n",
      "New data, epoch 68\n",
      "Step 0 Loss 0.167978\n",
      "Step 100 Loss 6.15067e-05\n",
      "Step 200 Loss 6.08684e-05\n",
      "Step 300 Loss 5.70888e-05\n",
      "Step 400 Loss 4.83874e-05\n",
      "Step 500 Loss 4.98017e-05\n",
      "Step 600 Loss 6.7151e-05\n",
      "New data, epoch 69\n",
      "Step 0 Loss 0.252452\n",
      "Step 100 Loss 5.97987e-05\n",
      "Step 200 Loss 6.81271e-05\n",
      "Step 300 Loss 4.89563e-05\n",
      "Step 400 Loss 5.43363e-05\n",
      "Step 500 Loss 4.76421e-05\n",
      "Step 600 Loss 8.19063e-05\n",
      "New data, epoch 70\n",
      "Step 0 Loss 0.251385\n",
      "Step 100 Loss 6.28784e-05\n",
      "Step 200 Loss 5.73768e-05\n",
      "Step 300 Loss 7.15916e-05\n",
      "Step 400 Loss 5.5085e-05\n",
      "Step 500 Loss 6.55788e-05\n",
      "Step 600 Loss 5.22607e-05\n",
      "New data, epoch 71\n",
      "Step 0 Loss 0.252349\n",
      "Step 100 Loss 5.02814e-05\n",
      "Step 200 Loss 5.23298e-05\n",
      "Step 300 Loss 6.02357e-05\n",
      "Step 400 Loss 6.21665e-05\n",
      "Step 500 Loss 6.49686e-05\n",
      "Step 600 Loss 5.91658e-05\n",
      "New data, epoch 72\n",
      "Step 0 Loss 0.535202\n",
      "Step 100 Loss 6.27266e-05\n",
      "Step 200 Loss 5.69076e-05\n",
      "Step 300 Loss 5.08176e-05\n",
      "Step 400 Loss 6.14715e-05\n",
      "Step 500 Loss 6.82346e-05\n",
      "Step 600 Loss 6.64846e-05\n",
      "New data, epoch 73\n",
      "Step 0 Loss 0.278501\n",
      "Step 100 Loss 7.02032e-05\n",
      "Step 200 Loss 6.61384e-05\n",
      "Step 300 Loss 7.46612e-05\n",
      "Step 400 Loss 8.30695e-05\n",
      "Step 500 Loss 6.02865e-05\n",
      "Step 600 Loss 7.10401e-05\n",
      "New data, epoch 74\n",
      "Step 0 Loss 0.467803\n",
      "Step 100 Loss 9.43011e-05\n",
      "Step 200 Loss 6.7708e-05\n",
      "Step 300 Loss 6.47354e-05\n",
      "Step 400 Loss 8.14353e-05\n",
      "Step 500 Loss 6.00451e-05\n",
      "Step 600 Loss 7.63797e-05\n",
      "New data, epoch 75\n",
      "Step 0 Loss 0.481739\n",
      "Step 100 Loss 9.61286e-05\n",
      "Step 200 Loss 7.74229e-05\n",
      "Step 300 Loss 7.58206e-05\n",
      "Step 400 Loss 5.71167e-05\n",
      "Step 500 Loss 5.25862e-05\n",
      "Step 600 Loss 6.87663e-05\n",
      "New data, epoch 76\n",
      "Step 0 Loss 0.228265\n",
      "Step 100 Loss 8.0485e-05\n",
      "Step 200 Loss 4.91616e-05\n",
      "Step 300 Loss 4.8904e-05\n",
      "Step 400 Loss 6.19013e-05\n",
      "Step 500 Loss 6.30106e-05\n",
      "Step 600 Loss 5.6536e-05\n",
      "New data, epoch 77\n",
      "Step 0 Loss 0.129436\n",
      "Step 100 Loss 5.77756e-05\n",
      "Step 200 Loss 6.51199e-05\n",
      "Step 300 Loss 5.7885e-05\n",
      "Step 400 Loss 4.80599e-05\n",
      "Step 500 Loss 6.26452e-05\n",
      "Step 600 Loss 5.59748e-05\n",
      "New data, epoch 78\n",
      "Step 0 Loss 0.394513\n",
      "Step 100 Loss 8.13259e-05\n",
      "Step 200 Loss 5.93775e-05\n",
      "Step 300 Loss 6.32188e-05\n",
      "Step 400 Loss 5.93037e-05\n",
      "Step 500 Loss 6.29267e-05\n",
      "Step 600 Loss 6.71527e-05\n",
      "New data, epoch 79\n",
      "Step 0 Loss 0.362683\n",
      "Step 100 Loss 4.66808e-05\n",
      "Step 200 Loss 5.74371e-05\n",
      "Step 300 Loss 5.77297e-05\n",
      "Step 400 Loss 6.45606e-05\n",
      "Step 500 Loss 6.25722e-05\n",
      "Step 600 Loss 5.78296e-05\n",
      "New data, epoch 80\n",
      "Step 0 Loss 0.320767\n",
      "Step 100 Loss 6.56826e-05\n",
      "Step 200 Loss 6.13368e-05\n",
      "Step 300 Loss 6.34827e-05\n",
      "Step 400 Loss 4.82741e-05\n",
      "Step 500 Loss 6.18838e-05\n",
      "Step 600 Loss 6.90314e-05\n",
      "New data, epoch 81\n",
      "Step 0 Loss 0.180238\n",
      "Step 100 Loss 5.08552e-05\n",
      "Step 200 Loss 6.4099e-05\n",
      "Step 300 Loss 5.88555e-05\n",
      "Step 400 Loss 5.61497e-05\n",
      "Step 500 Loss 6.35412e-05\n",
      "Step 600 Loss 6.70681e-05\n",
      "New data, epoch 82\n",
      "Step 0 Loss 0.139157\n",
      "Step 100 Loss 5.90584e-05\n",
      "Step 200 Loss 4.97178e-05\n",
      "Step 300 Loss 5.45175e-05\n",
      "Step 400 Loss 4.78758e-05\n",
      "Step 500 Loss 4.77041e-05\n",
      "Step 600 Loss 5.32605e-05\n",
      "New data, epoch 83\n",
      "Step 0 Loss 0.141658\n",
      "Step 100 Loss 5.31222e-05\n",
      "Step 200 Loss 5.96751e-05\n",
      "Step 300 Loss 4.58892e-05\n",
      "Step 400 Loss 4.45748e-05\n",
      "Step 500 Loss 5.77441e-05\n",
      "Step 600 Loss 5.28853e-05\n",
      "New data, epoch 84\n",
      "Step 0 Loss 0.152602\n",
      "Step 100 Loss 6.55934e-05\n",
      "Step 200 Loss 5.8384e-05\n",
      "Step 300 Loss 5.61574e-05\n",
      "Step 400 Loss 5.35835e-05\n",
      "Step 500 Loss 5.69761e-05\n",
      "Step 600 Loss 5.3198e-05\n",
      "New data, epoch 85\n",
      "Step 0 Loss 0.136875\n",
      "Step 100 Loss 4.74639e-05\n",
      "Step 200 Loss 5.51008e-05\n",
      "Step 300 Loss 5.546e-05\n",
      "Step 400 Loss 5.2396e-05\n",
      "Step 500 Loss 4.70286e-05\n",
      "Step 600 Loss 4.9999e-05\n",
      "New data, epoch 86\n",
      "Step 0 Loss 0.111062\n",
      "Step 100 Loss 5.21732e-05\n",
      "Step 200 Loss 5.02455e-05\n",
      "Step 300 Loss 4.86623e-05\n",
      "Step 400 Loss 4.12116e-05\n",
      "Step 500 Loss 3.76452e-05\n",
      "Step 600 Loss 4.68442e-05\n",
      "New data, epoch 87\n",
      "Step 0 Loss 0.277561\n",
      "Step 100 Loss 6.81511e-05\n",
      "Step 200 Loss 5.4253e-05\n",
      "Step 300 Loss 4.04215e-05\n",
      "Step 400 Loss 5.13782e-05\n",
      "Step 500 Loss 4.41422e-05\n",
      "Step 600 Loss 5.9791e-05\n",
      "New data, epoch 88\n",
      "Step 0 Loss 0.183544\n",
      "Step 100 Loss 5.11066e-05\n",
      "Step 200 Loss 5.23523e-05\n",
      "Step 300 Loss 6.50595e-05\n",
      "Step 400 Loss 4.6879e-05\n",
      "Step 500 Loss 6.09955e-05\n",
      "Step 600 Loss 4.77659e-05\n",
      "New data, epoch 89\n",
      "Step 0 Loss 0.249945\n",
      "Step 100 Loss 5.60435e-05\n",
      "Step 200 Loss 3.96175e-05\n",
      "Step 300 Loss 4.94049e-05\n",
      "Step 400 Loss 4.55887e-05\n",
      "Step 500 Loss 5.06461e-05\n",
      "Step 600 Loss 4.06919e-05\n",
      "New data, epoch 90\n",
      "Step 0 Loss 0.240735\n",
      "Step 100 Loss 5.81255e-05\n",
      "Step 200 Loss 3.98318e-05\n",
      "Step 300 Loss 5.477e-05\n",
      "Step 400 Loss 4.92648e-05\n",
      "Step 500 Loss 5.37689e-05\n",
      "Step 600 Loss 4.73146e-05\n",
      "New data, epoch 91\n",
      "Step 0 Loss 0.245547\n",
      "Step 100 Loss 5.8793e-05\n",
      "Step 200 Loss 6.24384e-05\n",
      "Step 300 Loss 4.9365e-05\n",
      "Step 400 Loss 4.7639e-05\n",
      "Step 500 Loss 4.63167e-05\n",
      "Step 600 Loss 4.32125e-05\n",
      "New data, epoch 92\n",
      "Step 0 Loss 0.328921\n",
      "Step 100 Loss 5.56132e-05\n",
      "Step 200 Loss 4.99957e-05\n",
      "Step 300 Loss 5.70872e-05\n",
      "Step 400 Loss 5.18265e-05\n",
      "Step 500 Loss 5.6737e-05\n",
      "Step 600 Loss 5.88798e-05\n",
      "New data, epoch 93\n",
      "Step 0 Loss 0.161655\n",
      "Step 100 Loss 4.51214e-05\n",
      "Step 200 Loss 4.60384e-05\n",
      "Step 300 Loss 4.24448e-05\n",
      "Step 400 Loss 5.39771e-05\n",
      "Step 500 Loss 4.27296e-05\n",
      "Step 600 Loss 5.61627e-05\n",
      "New data, epoch 94\n",
      "Step 0 Loss 0.179771\n",
      "Step 100 Loss 4.5618e-05\n",
      "Step 200 Loss 4.93552e-05\n",
      "Step 300 Loss 5.29012e-05\n",
      "Step 400 Loss 5.45049e-05\n",
      "Step 500 Loss 5.64616e-05\n",
      "Step 600 Loss 6.07174e-05\n",
      "New data, epoch 95\n",
      "Step 0 Loss 0.139214\n",
      "Step 100 Loss 5.01023e-05\n",
      "Step 200 Loss 5.2925e-05\n",
      "Step 300 Loss 5.39454e-05\n",
      "Step 400 Loss 5.70099e-05\n",
      "Step 500 Loss 4.43283e-05\n",
      "Step 600 Loss 5.04825e-05\n",
      "New data, epoch 96\n",
      "Step 0 Loss 0.121328\n",
      "Step 100 Loss 7.26218e-05\n",
      "Step 200 Loss 6.39964e-05\n",
      "Step 300 Loss 5.28542e-05\n",
      "Step 400 Loss 5.17313e-05\n",
      "Step 500 Loss 4.35139e-05\n",
      "Step 600 Loss 5.57094e-05\n",
      "New data, epoch 97\n",
      "Step 0 Loss 0.236937\n",
      "Step 100 Loss 4.58471e-05\n",
      "Step 200 Loss 5.11261e-05\n",
      "Step 300 Loss 4.48752e-05\n",
      "Step 400 Loss 4.24068e-05\n",
      "Step 500 Loss 3.61274e-05\n",
      "Step 600 Loss 5.40233e-05\n",
      "New data, epoch 98\n",
      "Step 0 Loss 0.190928\n",
      "Step 100 Loss 4.36845e-05\n",
      "Step 200 Loss 3.83428e-05\n",
      "Step 300 Loss 3.72605e-05\n",
      "Step 400 Loss 5.44589e-05\n",
      "Step 500 Loss 4.23131e-05\n",
      "Step 600 Loss 5.33795e-05\n",
      "New data, epoch 99\n",
      "Step 0 Loss 0.19589\n",
      "Step 100 Loss 5.13201e-05\n",
      "Step 200 Loss 4.65498e-05\n",
      "Step 300 Loss 6.39678e-05\n",
      "Step 400 Loss 4.79104e-05\n",
      "Step 500 Loss 4.16373e-05\n",
      "Step 600 Loss 5.28217e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAEACAYAAABF+UbAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x187296cfbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    plt.ion()\n",
    "    plt.figure()\n",
    "    plt.show()\n",
    "    loss_list=[]\n",
    "    \n",
    "    for epoch_idx in range(num_epochs):\n",
    "        x,y = generateData()\n",
    "        # _current_cell_state = np.zeros((batch_size, state_size))\n",
    "        # _current_hidden_state = np.zeros((batch_size, state_size))\n",
    "        # The following _current_state is defined for the multi-layered LSTM\n",
    "        # It defines as followed:\n",
    "        # (1) the number of layers we have in the RNN model, within each layers,\n",
    "        # (2) there are 2 states - cell_state and hidden_state\n",
    "        # (3) Each state will simultaneously consider batch_size of input, and \n",
    "        # (4) the number of units of the state is of state_size\n",
    "        _current_state = np.zeros((num_layers, 2, batch_size, state_size))\n",
    "        \n",
    "        print(\"New data, epoch\", epoch_idx)\n",
    "        \n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * truncated_backprop_length\n",
    "            end_idx = start_idx + truncated_backprop_length\n",
    "            \n",
    "            batchX = x[:,start_idx:end_idx]\n",
    "            batchY = y[:,start_idx:end_idx]\n",
    "            \n",
    "            _total_loss, _train_step, _current_state, _predictions_series = \\\n",
    "            sess.run(\n",
    "                [total_loss, train_step, current_state, predictions_series],\n",
    "                feed_dict={\n",
    "                    batchX_placeholder: batchX,\n",
    "                    batchY_placeholder: batchY,\n",
    "                    init_state: _current_state\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # current_state is a tuple of (cell_state, hidden_state)\n",
    "            # _current_cell_state, _current_hidden_state = _current_state\n",
    "            \n",
    "            \n",
    "            loss_list.append(_total_loss)\n",
    "            \n",
    "            if batch_idx%100 == 0:\n",
    "                print(\"Step\",batch_idx,\"Loss\", _total_loss)\n",
    "                plot(loss_list, _predictions_series, batchX, batchY)\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "current_input = inputs_series[0]\n",
    "current_input = tf.reshape(current_input, [batch_size, 1])\n",
    "current_state = init_state\n",
    "tf.concat([current_input, current_state], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
